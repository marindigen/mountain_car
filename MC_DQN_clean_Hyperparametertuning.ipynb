{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc71e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   5%|█▉                                     | 150/3000 [00:44<14:42,  3.23it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\"0. initialize libraries and setup tools\"\"\"\n",
    "\n",
    "%load_ext jupyternotify\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()  # checks if one is running IPhyton environment like jupyter notebook\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()  # interactive mode on, allows automatic plots when data is updated (whithout calling plt.show every time)\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" 1.1. Helper functions \"\"\"\n",
    "\n",
    "def plot_durations(durations, hyperparameters):\n",
    "    plt.figure()\n",
    "    durations_t = torch.tensor(durations, dtype=torch.float)\n",
    "    plt.title('Duration per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        plt.plot(range(100, len(durations)-99), means.numpy(), label='100 Moving Average')\n",
    "    plt.savefig(f\"DQN_Duration_Hyperparameters_{hyperparameters}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_curve(data, title, xlabel, ylabel, savefig=False):\n",
    "    plt.plot(data, label = 'title')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if savefig: \n",
    "        plt.savefig(f\"{title}.png\", dpi=300) # Use f-string to concatenate strings and variable\n",
    "        \n",
    "def plot_scatter(data, xlabel, ylabel):\n",
    "    data_t = torch.tensor(data, dtype=torch.float)\n",
    "    plt.title('Result')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.scatter(list(range(len(data))), data_t.numpy()) # usually for plt.scatter you need (x,y) as arguments\n",
    "\n",
    "def plot_with_average(data, xlabel, ylabel):\n",
    "    plt.figure(1)\n",
    "    data_t = torch.tensor(data, dtype=torch.float)\n",
    "    plt.title('Result')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(list(range(len(data))), data_t.numpy(), label='Cumulative Reward Per Episode') \n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(data_t) > 100:  # Check if data has more than 100 episodes\n",
    "        # Calculate moving average starting from episode 100\n",
    "        means = data_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        # Plot the average line starting from episode 100\n",
    "        plt.plot(range(100, len(data)-99), means.numpy(), label='Moving Average (100 episodes)')\n",
    "\n",
    "def cumulative_sum(input_list):\n",
    "    result = []\n",
    "    running_total = 0\n",
    "    for element in input_list:\n",
    "        running_total += element\n",
    "        result.append(running_total)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\"1.2. Define ReplayBuffer and deep q network\"\"\"\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args)) #Save a transition\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, n_nodes_per_layer=64, n_layers=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(n_observations, n_nodes_per_layer)])\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(n_nodes_per_layer, n_nodes_per_layer))\n",
    "        self.output_layer = nn.Linear(n_nodes_per_layer, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output_layer(x)\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "\"\"\"\" 2. Agent class \"\"\"\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, n_actions, n_observations):\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.EPS_START = EPS_START\n",
    "        self.EPS_END = EPS_END\n",
    "        self.EPS_DECAY = EPS_DECAY\n",
    "        self.TAU = TAU\n",
    "        self.LR = LR\n",
    "        self.n_actions = n_actions\n",
    "        self.n_observations = n_observations\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.loss_per_episode = []\n",
    "        self.cumulative_reward_per_episode = []\n",
    "        self.cumulative_environment_reward_per_episode = []\n",
    "        self.cumulative_auxiliary_reward_per_episode = []\n",
    "        self.agent_performance = []\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "        expected_state_action_values = expected_state_action_values.float()\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def update(self, num_episodes):\n",
    "        for episode in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            cumulative_reward_t = 0\n",
    "            environment_reward_t = 0\n",
    "            auxiliary_reward_t = 0\n",
    "            running_loss = []\n",
    "            for t in count():\n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                environment_reward_t += reward\n",
    "                aux_reward = 0.1*observation[0]\n",
    "                auxiliary_reward_t += aux_reward\n",
    "                reward = reward + aux_reward\n",
    "                cumulative_reward_t += reward\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if not done:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    self.agent_performance.append(1)\n",
    "                else:\n",
    "                    next_state = None\n",
    "                if truncated:\n",
    "                    self.agent_performance.append(0)\n",
    "                    \n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "                \n",
    "                loss_value = self.optimize_model()\n",
    "                \n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    break\n",
    "                    \n",
    "                running_loss.append(loss_value)\n",
    "            \n",
    "            filtered_running_loss = [loss for loss in running_loss if loss is not None]\n",
    "            if filtered_running_loss:\n",
    "                self.loss_per_episode.append(np.mean(filtered_running_loss))\n",
    "\n",
    "            self.cumulative_reward_per_episode.append(cumulative_reward_t)\n",
    "            self.cumulative_environment_reward_per_episode.append(environment_reward_t)\n",
    "            self.cumulative_auxiliary_reward_per_episode.append(auxiliary_reward_t)\n",
    "        \n",
    "    \n",
    "\n",
    "\"\"\"\" 3. Run Model \"\"\"\n",
    "\n",
    "def train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, num_episodes=3000):\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = env.observation_space.shape[0]\n",
    "    agent = DQNAgent(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, n_actions, n_observations)\n",
    "    agent.update(num_episodes)\n",
    "    hyperparameters = f\"BATCH_SIZE={BATCH_SIZE}_GAMMA={GAMMA}_EPS_START={EPS_START}_EPS_END={EPS_END}_EPS_DECAY={EPS_DECAY}_TAU={TAU}_LR={LR}\"\n",
    "    plot_durations(agent.episode_durations, hyperparameters)\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.1\n",
    "    EPS_DECAY = 100000\n",
    "    TAU = 0.0005 \n",
    "    LR = 1e-4\n",
    "\n",
    "#     for BATCH_SIZE in [32, 64, 128]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for GAMMA in [0.95, 0.90]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "#     for EPS_START in [1.0, 0.9, 0.8]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "#     for EPS_END in [0.1, 0.05, 0.01]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "#     for EPS_DECAY in [10000, 100000, 1000000]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "#     for TAU in [0.00005, 0.0005, 0.005]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "#     for LR in [0.001, 0.0005, 0.0001]:\n",
    "#         train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the hyperparameter tuning\n",
    "hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Duration per Episode =====================\n",
    "\n",
    "# figure_title = 'DQN with position as auxiliary reward' # = filename\n",
    "# plot_durations(figure_title, savefig=False, show_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.episode_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Episode Durations, Scatterplot =====================\n",
    "\n",
    "# plot_scatter(episode_durations, 'Episode', 'Duration')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Loss Curve =====================\n",
    "\n",
    "plot_curve(agent.loss_per_episode, 'Loss Curve', 'Training Step', 'Loss')\n",
    "plt.show()\n",
    "\n",
    "# report loss\n",
    "print(\"final loss:\", loss_per_episode[-1])\n",
    "\n",
    "# report average cumulative reward per ep\n",
    "average_cumulative_reward_per_episode = np.mean(cumulative_reward_per_episode)\n",
    "print('average_cumulative_reward_per_episode = ', average_cumulative_reward_per_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Cumulative Reward per Episode =====================\n",
    "\n",
    "#plot_with_average(cumulative_reward_per_episode, 'Episode', 'Cumulative Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Cumulative Reward per Episode, Scatterplot =====================\n",
    "\n",
    "# plot_scatter(cumulative_reward_per_episode, 'Episode', 'Cumulative Reward')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Composition of Averaged Cumulative Reward per Episode =====================\n",
    "\n",
    "data_t = torch.tensor(cumulative_reward_per_episode, dtype=torch.float)\n",
    "plt.title('Composition of Averaged Cumulative Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "c_e_r_p_e_t = torch.tensor(cumulative_environment_reward_per_episode, dtype=torch.float)\n",
    "c_a_r_p_e_t = torch.tensor(cumulative_auxiliary_reward_per_episode, dtype=torch.float)\n",
    "if len(c_e_r_p_e_t) > 100:  # Check if data has more than 100 episodes\n",
    "        means = data_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        # Plot the average line starting from episode 100\n",
    "        plt.plot(range(100, len(cumulative_reward_per_episode)-99), means.numpy(), label='Cumulative Reward Per Episode')\n",
    "        # Calculate moving average starting from episode 100\n",
    "        means = c_e_r_p_e_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        # Plot the average line starting from episode 100\n",
    "        plt.plot(range(100, len(cumulative_environment_reward_per_episode)-99), means.numpy(), label='Cumulative Environment Reward Per Episode')\n",
    "        means = c_a_r_p_e_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        # Plot the average line starting from episode 100\n",
    "        plt.plot(range(100, len(cumulative_auxiliary_reward_per_episode)-99), means.numpy(), label='Cumulative Auxiliary Reward Per Episode')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============ Cumulative number of successes per Episode =====================\n",
    "\n",
    "# cumulative_reward = cumulative_sum(cumulative_reward_per_episode)\n",
    "# cumulative_environment_reward = cumulative_sum(cumulative_environment_reward_per_episode)\n",
    "# cumulative_auxiliary_reward = cumulative_sum(cumulative_auxiliary_reward_per_episode)\n",
    "\n",
    "# #separately depict the environment reward, the auxiliary reward and their sum\n",
    "# #### SMOOTHEN IF NECESSARY\n",
    "# plt.figure(1)\n",
    "# plt.title('Composition of Cumulative Reward')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Cumulative Reward')\n",
    "# plt.plot(cumulative_reward, label='Cumulative Reward')\n",
    "# plt.plot(cumulative_environment_reward, label='Environment Reward')\n",
    "# plt.plot(cumulative_auxiliary_reward, label='Auxiliary Reward')\n",
    "\n",
    "# plt.legend()  # Show legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Cumulative number of successes =====================\n",
    "\n",
    "cumulative_successes = cumulative_sum(agent_performance)\n",
    "plot_curve(cumulative_successes, 'Cumulative Number of Successes', 'Episodes', 'Successes', savefig=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
