{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cce69e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\"0. initialize libraries and setup tools\"\"\"\n",
    "\n",
    "%load_ext jupyternotify\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()  # checks if one is running IPhyton environment like jupyter notebook\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()  # interactive mode on, allows automatic plots when data is updated (whithout calling plt.show every time)\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\" 1.1. Helper functions \"\"\"\n",
    "    \n",
    "def plot_durations(episode_durations, hyperparameters):\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Duration per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.scatter(list(range(len(episode_durations))), durations_t.numpy(), color='grey')\n",
    "    \n",
    "    moving_avg = []\n",
    "    for i in range(len(episode_durations)):\n",
    "        if i < 100:\n",
    "            moving_avg.append(durations_t[:i+1].mean().item())  \n",
    "        else:\n",
    "            moving_avg.append(durations_t[i-99:i+1].mean().item()) \n",
    "    \n",
    "    plt.plot(range(len(episode_durations)), moving_avg, color='orange', label='100-Moving Average')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"DQN_Duration_Hyperparameters_{hyperparameters}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_curve(data, title, xlabel, ylabel, hyperparameters):\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"DQN_{title}_Hyperparameters_{hyperparameters}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_cumul_reward(data, xlabel, ylabel, hyperparameters):\n",
    "    plt.figure(1)\n",
    "    data_t = torch.tensor(data, dtype=torch.float)\n",
    "    plt.title('Cumulative Reward Per Episode')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.scatter(list(range(len(data))), data_t.numpy(), color = 'grey', label='Cumulative Reward Per Episode') \n",
    "    if len(data_t) > 100:\n",
    "        means = data_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "        plt.plot(range(100, len(data)-99), means.numpy(), color = 'orange', label='Moving Average (100 episodes)')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"DQN_Cumul_Reward_Hyperparameters_{hyperparameters}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "def cumulative_sum(input_list):\n",
    "    result = []\n",
    "    running_total = 0\n",
    "    for element in input_list:\n",
    "        running_total += element\n",
    "        result.append(running_total)\n",
    "    return result\n",
    "\n",
    "def plot_comp_cumul_reward(data, data_env, data_aux, hyperparameters):\n",
    "    data_t = torch.tensor(data, dtype=torch.float)\n",
    "    plt.title('Composition of Averaged Cumulative Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    c_e_r_p_e_t = torch.tensor(data_env, dtype=torch.float)\n",
    "    c_a_r_p_e_t = torch.tensor(data_aux, dtype=torch.float)\n",
    "    if len(c_e_r_p_e_t) > 100:  # Check if data has more than 100 episodes\n",
    "            means = data_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "            # Plot the average line starting from episode 100\n",
    "            plt.plot(range(100, len(data)-99), means.numpy(), label='Cumulative Reward Per Episode')\n",
    "            # Calculate moving average starting from episode 100\n",
    "            means = c_e_r_p_e_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "            # Plot the average line starting from episode 100\n",
    "            plt.plot(range(100, len(data_env)-99), means.numpy(), label='Cumulative Environment Reward Per Episode')\n",
    "            means = c_a_r_p_e_t[100:].unfold(0, 100, 1).mean(1).view(-1)\n",
    "            # Plot the average line starting from episode 100\n",
    "            plt.plot(range(100, len(data_aux)-99), means.numpy(), label='Cumulative Auxiliary Reward Per Episode')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"DQN_Comp_Cumul_Reward_Hyperparameters_{hyperparameters}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\"1.2. Define ReplayBuffer and deep q network\"\"\"\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, n_nodes_per_layer=64, n_layers=2):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(n_observations, n_nodes_per_layer)])\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(n_nodes_per_layer, n_nodes_per_layer))\n",
    "        self.output_layer = nn.Linear(n_nodes_per_layer, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output_layer(x)\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "\"\"\"\" 2. Agent class \"\"\"\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, n_actions, n_observations):\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.EPS_START = EPS_START\n",
    "        self.EPS_END = EPS_END\n",
    "        self.EPS_DECAY = EPS_DECAY\n",
    "        self.TAU = TAU\n",
    "        self.LR = LR\n",
    "        self.n_actions = n_actions\n",
    "        self.n_observations = n_observations\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.loss_per_episode = []\n",
    "        self.cumulative_reward_per_episode = []\n",
    "        self.cumulative_environment_reward_per_episode = []\n",
    "        self.cumulative_auxiliary_reward_per_episode = []\n",
    "        self.agent_performance = []\n",
    "    \n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "        expected_state_action_values = expected_state_action_values.float()\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def update(self, num_episodes):\n",
    "        for episode in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
    "            state, info = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            cumulative_reward_t = 0\n",
    "            environment_reward_t = 0\n",
    "            auxiliary_reward_t = 0\n",
    "            running_loss = []\n",
    "            for t in count():\n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                environment_reward_t += reward\n",
    "                aux_reward = 0 # 3*(observation[0] + 0.5)**2\n",
    "                auxiliary_reward_t += aux_reward\n",
    "                reward = reward + aux_reward\n",
    "                cumulative_reward_t += reward\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                    self.agent_performance.append(1)\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "                if truncated:\n",
    "                    self.agent_performance.append(0)\n",
    "                    \n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "                \n",
    "                loss_value = self.optimize_model()\n",
    "                \n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    break\n",
    "                    \n",
    "                running_loss.append(loss_value)\n",
    "            \n",
    "            filtered_running_loss = [loss for loss in running_loss if loss is not None]\n",
    "            if filtered_running_loss:\n",
    "                self.loss_per_episode.append(np.mean(filtered_running_loss))\n",
    "\n",
    "            self.cumulative_reward_per_episode.append(cumulative_reward_t)\n",
    "            self.cumulative_environment_reward_per_episode.append(environment_reward_t)\n",
    "            self.cumulative_auxiliary_reward_per_episode.append(auxiliary_reward_t)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc71e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|█████████████████████████████| 1000/1000 [04:26<00:00,  3.76it/s]\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"bb7ca2d6-bab7-4418-a8f5-f0c494907ec0\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"bb7ca2d6-bab7-4418-a8f5-f0c494907ec0\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "\"\"\"\" 3. Run Model \"\"\"\n",
    "\n",
    "def train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, num_episodes=1000):\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = env.observation_space.shape[0]\n",
    "    agent = DQNAgent(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR, n_actions, n_observations)\n",
    "    agent.update(num_episodes)\n",
    "    hyperparameters = f\"BATCH_SIZE={BATCH_SIZE}_GAMMA={GAMMA}_EPS_START={EPS_START}_EPS_END={EPS_END}_EPS_DECAY={EPS_DECAY}_TAU={TAU}_LR={LR}\"\n",
    "    \n",
    "    # choose what you want to plot\n",
    "    plot_durations(agent.episode_durations, hyperparameters)\n",
    "    plot_curve(agent.loss_per_episode, 'Loss Curve', 'Training Step', 'Loss', hyperparameters)\n",
    "    plot_cumul_reward(agent.cumulative_reward_per_episode, 'Episode', 'Cumulative Reward', hyperparameters)\n",
    "    plot_comp_cumul_reward(agent.cumulative_reward_per_episode, agent.cumulative_environment_reward_per_episode, agent.cumulative_auxiliary_reward_per_episode, hyperparameters)\n",
    "    cumulative_successes = cumulative_sum(agent.agent_performance)\n",
    "    plot_curve(cumulative_successes, 'Cumulative Number of Successes', 'Episodes', 'Successes', hyperparameters)\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.1\n",
    "    EPS_DECAY = 100000\n",
    "    TAU = 0.0005 # maybe also 0.005\n",
    "    LR = 1e-4\n",
    "\n",
    "    for BATCH_SIZE in [32, 64, 128]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for GAMMA in [0.9, 0.95, 0.999, 0.99]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for EPS_START in [1.0, 0.8, 0.9]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for EPS_END in [0.01, 0.05, 0.1]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for EPS_DECAY in [10000, 1000000, 100000]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for TAU in [0.00005, 0.005, 0.0005]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "    for LR in [0.001, 0.0005, 0.0001]:\n",
    "        train_agent_with_hyperparameters(BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TAU, LR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the hyperparameter tuning\n",
    "#hyperparameter_tuning()\n",
    "\n",
    "# Run the agent training\n",
    "train_agent_with_hyperparameters(64, 0.99, 0.9, 0.1, 100000, 0.0005, 1e-4, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b09e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e13079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
