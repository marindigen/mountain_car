{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Miniproject Tutorial Notebook\n",
    "\n",
    "This notebook is here to guide you through the basics of the frameworks necessary for you to do well on your CS456-Miniproject ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium environments\n",
    "\n",
    "One of the main and most spread environment developer in the field of RL research is [Gymnasium](https://gymnasium.farama.org/). They provide standardized environments offering a large range of difficulties and setups, that are perfectly designed to benchmark performances of RL and Deep RL algorithms.\n",
    "\n",
    "The main structure is very simple to understand. First, we need to instantiate our environment. We will use an existing environment, but one could also use their structure to design their own environment.\n",
    "\n",
    "Let's directly work with the Mountain Car environment that will be used in the project. \n",
    "\n",
    "_PS: If you're more curious, feel free to browse the large list available on their website!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains an action space and an observation (state) space. Let's see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions available: 3\n",
      "Observation shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions available: {env.action_space.n}\")\n",
    "print(f\"Observation shape: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the action space of that first environment is discrete and contains 3 possible actions: accelerate to the left, don't accelerate and accelerate to the right. \n",
    "\n",
    "The observation space has a dimension of 2, and you can find what each part represents [here](https://gymnasium.farama.org/environments/classic_control/mountain_car/#observation-space).\n",
    "\n",
    "Before taking actions, the environment should be reset (or boostrapped). **Note: this should be done every time the environment has to be restarted, i.e., at the end of any episode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.42079636  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# the second return value is an info dictionary, but it doesn't contain anything in this environment\n",
    "starting_state, _ = env.reset() \n",
    "\n",
    "print(f\"Starting state: {starting_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the actions look like and that the environment is ready, we can take actions inside it. This is done using the `env.step` function, that takes an action as input, and returns multiple values. More details on each of them can be found [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.step).\n",
    "\n",
    "In the project, you will have an agent that will choose an action (based on the policy learned) given the current state. However, for now, we can simply sample actions at random using `action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n",
      "Next state: [-0.42255524 -0.00175885]\n",
      "Reward: -1.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")\n",
    "next_state, reward, terminated, truncated, _ = env.step(action) # again, the last return value is an empty info object\n",
    "\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminated` and `truncated`  variables represent the two ways that the episode might be done. Thus, it might be handy to use\n",
    "```\n",
    "done = terminated or truncated\n",
    "```\n",
    "in your code. ðŸ’¡\n",
    "\n",
    "We now have all the pieces necessary to run a full episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward after taking random actions: -200.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Episode reward after taking random actions: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your goal in the project will be to code an agent that can beat that ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Dyna\n",
    "\n",
    "4.1 State discretization\n",
    "In the mountain car environment, the state ranges from [âˆ’1.2, âˆ’0.07] to [0.6, 0.07]. To create a discrete state space, we divide the state space into bins. The bin sizes should differ for the two state dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 9]\n"
     ]
    }
   ],
   "source": [
    "num_bins = np.array([18,14]) # position and velocity\n",
    "\n",
    "def discretize_state(state, state_space, num_bins):\n",
    "    bin_sizes = (state_space.high - state_space.low) / num_bins\n",
    "    return ((state - state_space.low) / bin_sizes).astype(int), bin_sizes\n",
    "\n",
    "state = np.array([-0.5, 0.03])\n",
    "discrete_state,__ = discretize_state(state, env.observation_space, num_bins)\n",
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Model building**\n",
    "During training, the agent builds a model of the environment consisting of two key components: an estimation of transition probabilities PË†s,a(sâ€²) and an estimation of the reward for each state-action pair RË†(s, a). PË†s,a(sâ€²) represents the expected probability of transitioning to state sâ€² after taking action a in state s, while RË†(s, a) is the expected reward when taking action a in state s. These estimates are updated with each new observation.\n",
    "\n",
    "**4.3 Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "\n",
    "class DynaAgent:\n",
    "    def __init__(self, state_space, action_space, discr_step=np.array([0.025, 0.005]), gamma=0.99, epsilon=0.9, k=10):\n",
    "        self.discr_step = discr_step\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon \n",
    "        self.k = k\n",
    "        self.nstates = ((state_space.high - state_space.low) / self.discr_step).astype(int) + 1 # 1 is added to count upper bound as a separate state\n",
    "        self.nactions = action_space.n\n",
    "        self.P = np.ones((self.nstates[0], self.nstates[1], self.nactions, self.nstates[0], self.nstates[1])) / np.prod(self.nstates)\n",
    "        self.R = np.zeros((self.nstates[0], self.nstates[1], self.nactions))\n",
    "        self.Q = np.zeros((self.nstates[0], self.nstates[1], self.nactions))\n",
    "        self.visits = np.zeros((self.nstates[0], self.nstates[1], self.nactions), dtype=int)\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        return np.floor((state - state_space.low) / self.discr_step).astype(int)\n",
    "\n",
    "    def update_model(self, s, a, r, s_prime):\n",
    "        s_discrete = s #self.discretize_state(s)\n",
    "        s_prime_discrete = s_prime #self.discretize_state(s_prime)\n",
    "        self.visits[s_discrete[0], s_discrete[1], a] += 1\n",
    "        self.learning_rate = 1 / self.visits[s_discrete[0], s_discrete[1], a]\n",
    "        self.R[s_discrete[0], s_discrete[1], a] += self.learning_rate * (r - self.R[s_discrete[0], s_discrete[1], a])\n",
    "        self.P[s_discrete[0], s_discrete[1], a, s_prime_discrete[0], s_prime_discrete[1]] += self.learning_rate # controls how much the Q-values are updated at each step while moving towards the minimum of a loss function\n",
    "\n",
    "    def update_Q_value(self, s, a):\n",
    "        s_discrete =  s #self.discretize_state(s)\n",
    "        future_rewards = self.P[s_discrete[0], s_discrete[1], a] * np.max(self.Q, axis=2)\n",
    "        self.Q[s_discrete[0], s_discrete[1], a] = self.R[s_discrete[0], s_discrete[1], a] + self.gamma * np.sum(future_rewards)\n",
    "\n",
    "    def simulate(self):\n",
    "        s_prime = np.zeros(2, dtype=int)\n",
    "        for _ in range(self.k):\n",
    "            # Randomly select a state-action pair from those that have been visited\n",
    "            visited_states = np.where(self.visits > 0)\n",
    "            idx = np.random.choice(len(visited_states[0]))\n",
    "            s = (visited_states[0][idx], visited_states[1][idx])\n",
    "            a = visited_states[2][idx]\n",
    "\n",
    "            # Retrieve the model for this state-action pair\n",
    "            # fix transition probabilities\n",
    "            transition_probs= self.P[s[0], s[1], a]/ np.sum(self.P[s[0], s[1], a])\n",
    "            # Pick a new state randomly based on the transition probabilities\n",
    "            mat = np.repeat(np.arange(self.nstates[0])[:, np.newaxis], self.nstates[1], axis=1)\n",
    "            chosen_index = np.random.choice(mat.flatten(), p=transition_probs.flatten())\n",
    "            s_prime = np.unravel_index(chosen_index, shape=self.nstates)\n",
    "            # Retrieve the reward for this state-action pair\n",
    "            reward = self.R[s[0], s[1], a]\n",
    "\n",
    "            # Find the maximum Q-value for the next state across all possible actions\n",
    "            max_q_next = np.max(self.Q[s_prime[0], s_prime[1]])\n",
    "\n",
    "            # Update Q-value using the learning rate, reward, and discount factor\n",
    "            self.Q[s[0], s[1], a] += self.learning_rate * (reward + self.gamma * max_q_next - self.Q[s[0], s[1], a])\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            return np.random.randint(self.nactions)\n",
    "        else:\n",
    "            s_discrete = state #self.discretize_state(state)\n",
    "            return np.argmax(self.Q[s_discrete[0], s_discrete[1]])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= 0.99  # Exponential decay\n",
    "        self.epsilon = max(self.epsilon, 0.05)  # Ensuring epsilon does not go below 0.05\n",
    "\n",
    "# Usage\n",
    "state_space = env.observation_space\n",
    "action_space = env.action_space  # Assuming 3 possible actions\n",
    "agent = DynaAgent(state_space, action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[23 13]\n",
      "-1.0\n",
      "[23 13]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 13]\n",
      "-1.0\n",
      "[19 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[19 14]\n",
      "-1.0\n",
      "[20 14]\n",
      "-1.0\n",
      "[20 15]\n",
      "-1.0\n",
      "[20 15]\n",
      "-1.0\n",
      "[20 15]\n",
      "-1.0\n",
      "[21 15]\n",
      "-1.0\n",
      "[21 15]\n",
      "-1.0\n",
      "[21 15]\n",
      "-1.0\n",
      "[21 15]\n",
      "-1.0\n",
      "[21 15]\n",
      "-1.0\n",
      "[22 15]\n",
      "-1.0\n",
      "[22 15]\n",
      "-1.0\n",
      "[22 15]\n",
      "-1.0\n",
      "[22 15]\n",
      "-1.0\n",
      "[22 14]\n",
      "-1.0\n",
      "[23 15]\n",
      "-1.0\n",
      "[23 15]\n",
      "-1.0\n",
      "[23 15]\n",
      "-1.0\n",
      "[24 15]\n",
      "-1.0\n",
      "[24 15]\n",
      "-1.0\n",
      "[24 15]\n",
      "-1.0\n",
      "[24 15]\n",
      "-1.0\n",
      "[24 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 14]\n",
      "-1.0\n",
      "[25 13]\n",
      "-1.0\n",
      "[25 13]\n",
      "-1.0\n",
      "[25 13]\n",
      "-1.0\n",
      "[25 13]\n",
      "-1.0\n",
      "[25 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 13]\n",
      "-1.0\n",
      "[24 12]\n",
      "-1.0\n",
      "[24 12]\n",
      "-1.0\n",
      "[24 12]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[23 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[22 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[21 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[20 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[19 12]\n",
      "-1.0\n",
      "[18 12]\n",
      "-1.0\n",
      "[18 12]\n",
      "-1.0\n",
      "[18 12]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[18 13]\n",
      "-1.0\n",
      "[17 13]\n",
      "-1.0\n",
      "[17 13]\n",
      "-1.0\n",
      "[17 13]\n",
      "-1.0\n",
      "[17 13]\n",
      "-1.0\n",
      "[17 13]\n",
      "-1.0\n",
      "[17 14]\n",
      "-1.0\n",
      "[17 14]\n",
      "-1.0\n",
      "[17 14]\n",
      "-1.0\n",
      "[17 14]\n",
      "-1.0\n",
      "[17 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 14]\n",
      "-1.0\n",
      "[18 15]\n",
      "-1.0\n",
      "[19 15]\n",
      "-1.0\n",
      "[19 15]\n",
      "-1.0\n",
      "[19 15]\n",
      "-1.0\n",
      "[19 15]\n",
      "-1.0\n",
      "[20 15]\n",
      "-1.0\n",
      "[20 15]\n",
      "-1.0\n",
      "[21 16]\n",
      "-1.0\n",
      "[21 16]\n",
      "-1.0\n",
      "[21 16]\n",
      "-1.0\n",
      "[22 16]\n",
      "-1.0\n",
      "[22 16]\n",
      "-1.0\n",
      "[23 16]\n",
      "-1.0\n",
      "[23 15]\n",
      "-1.0\n",
      "[23 16]\n",
      "-1.0\n",
      "[24 16]\n",
      "-1.0\n",
      "[24 16]\n",
      "-1.0\n",
      "[25 16]\n",
      "-1.0\n",
      "[25 16]\n",
      "-1.0\n",
      "[26 16]\n",
      "-1.0\n",
      "[26 15]\n",
      "-1.0\n",
      "[26 15]\n",
      "-1.0\n",
      "[27 15]\n",
      "-1.0\n",
      "[27 15]\n",
      "-1.0\n",
      "[27 15]\n",
      "-1.0\n",
      "[27 14]\n",
      "-1.0\n",
      "[28 14]\n",
      "-1.0\n",
      "[28 14]\n",
      "-1.0\n",
      "[28 14]\n",
      "-1.0\n",
      "[28 13]\n",
      "-1.0\n",
      "[28 13]\n",
      "-1.0\n",
      "[28 13]\n",
      "-1.0\n",
      "[27 13]\n",
      "-1.0\n",
      "[27 13]\n",
      "-1.0\n",
      "[27 12]\n",
      "-1.0\n",
      "[27 12]\n",
      "-1.0\n",
      "[26 12]\n",
      "-1.0\n",
      "[26 12]\n",
      "-1.0\n",
      "[26 12]\n",
      "-1.0\n",
      "[25 11]\n",
      "-1.0\n",
      "[25 11]\n",
      "-1.0\n",
      "[24 11]\n",
      "-1.0\n",
      "[24 11]\n",
      "-1.0\n",
      "[23 11]\n",
      "-1.0\n",
      "[23 11]\n",
      "-1.0\n",
      "[22 11]\n",
      "-1.0\n",
      "[22 11]\n",
      "-1.0\n",
      "[21 11]\n",
      "-1.0\n",
      "[20 11]\n",
      "-1.0\n",
      "[20 11]\n",
      "-1.0\n",
      "[19 11]\n",
      "-1.0\n",
      "[19 11]\n",
      "-1.0\n",
      "[18 11]\n",
      "-1.0\n",
      "[18 11]\n",
      "-1.0\n",
      "[18 11]\n",
      "-1.0\n",
      "[17 11]\n",
      "-1.0\n",
      "[17 11]\n",
      "-1.0\n",
      "[16 12]\n",
      "-1.0\n",
      "[16 12]\n",
      "-1.0\n",
      "[16 12]\n",
      "-1.0\n",
      "[15 12]\n",
      "-1.0\n",
      "[15 12]\n",
      "-1.0\n",
      "[15 12]\n",
      "-1.0\n",
      "[15 13]\n",
      "-1.0\n",
      "[15 13]\n",
      "-1.0\n",
      "[14 13]\n",
      "-1.0\n",
      "[14 13]\n",
      "-1.0\n",
      "[14 13]\n",
      "-1.0\n",
      "[14 14]\n",
      "-1.0\n",
      "[14 14]\n",
      "-1.0\n",
      "[15 14]\n",
      "-1.0\n",
      "[15 14]\n",
      "-1.0\n",
      "[15 14]\n",
      "-1.0\n",
      "[15 15]\n",
      "-1.0\n",
      "[15 15]\n",
      "-1.0\n",
      "[16 15]\n",
      "-1.0\n",
      "[16 16]\n",
      "-1.0\n",
      "[17 16]\n",
      "-1.0\n",
      "[17 16]\n",
      "-1.0\n",
      "[18 16]\n",
      "-1.0\n",
      "Total reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "total_reward = 0\n",
    "steps_per_episode = 200\n",
    "\n",
    "agent = DynaAgent(env.observation_space, env.action_space)\n",
    "\n",
    "while not done:\n",
    "    state, _ = env.reset()\n",
    "    state = agent.discretize_state(state)\n",
    "    total_reward = 0\n",
    "    for t in range(steps_per_episode):\n",
    "        # Direct learning from environment interactions\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "        \n",
    "        next_state = agent.discretize_state(next_state)\n",
    "        agent.update_Q_value(state, action) # ask why this is done before updating the model or try different configurations\n",
    "        agent.update_model(state, action, episode_reward, next_state)\n",
    "\n",
    "        # Planning\n",
    "        agent.simulate()\n",
    "\n",
    "        total_reward += reward\n",
    "        print(next_state)\n",
    "        state = next_state\n",
    "        print(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
