{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Miniproject Tutorial Notebook\n",
    "\n",
    "This notebook is here to guide you through the basics of the frameworks necessary for you to do well on your CS456-Miniproject ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium environments\n",
    "\n",
    "One of the main and most spread environment developer in the field of RL research is [Gymnasium](https://gymnasium.farama.org/). They provide standardized environments offering a large range of difficulties and setups, that are perfectly designed to benchmark performances of RL and Deep RL algorithms.\n",
    "\n",
    "The main structure is very simple to understand. First, we need to instantiate our environment. We will use an existing environment, but one could also use their structure to design their own environment.\n",
    "\n",
    "Let's directly work with the Mountain Car environment that will be used in the project. \n",
    "\n",
    "_PS: If you're more curious, feel free to browse the large list available on their website!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains an action space and an observation (state) space. Let's see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions available: 3\n",
      "Observation shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions available: {env.action_space.n}\")\n",
    "print(f\"Observation shape: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the action space of that first environment is discrete and contains 3 possible actions: accelerate to the left, don't accelerate and accelerate to the right. \n",
    "\n",
    "The observation space has a dimension of 2, and you can find what each part represents [here](https://gymnasium.farama.org/environments/classic_control/mountain_car/#observation-space).\n",
    "\n",
    "Before taking actions, the environment should be reset (or boostrapped). **Note: this should be done every time the environment has to be restarted, i.e., at the end of any episode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.5626615  0.       ]\n"
     ]
    }
   ],
   "source": [
    "# the second return value is an info dictionary, but it doesn't contain anything in this environment\n",
    "starting_state, _ = env.reset() \n",
    "\n",
    "print(f\"Starting state: {starting_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the actions look like and that the environment is ready, we can take actions inside it. This is done using the `env.step` function, that takes an action as input, and returns multiple values. More details on each of them can be found [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.step).\n",
    "\n",
    "In the project, you will have an agent that will choose an action (based on the policy learned) given the current state. However, for now, we can simply sample actions at random using `action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n",
      "Next state: [-0.5633692 -0.0007077]\n",
      "Reward: -1.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")\n",
    "next_state, reward, terminated, truncated, _ = env.step(action) # again, the last return value is an empty info object\n",
    "\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminated` and `truncated`  variables represent the two ways that the episode might be done. Thus, it might be handy to use\n",
    "```\n",
    "done = terminated or truncated\n",
    "```\n",
    "in your code. ðŸ’¡\n",
    "\n",
    "We now have all the pieces necessary to run a full episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward after taking random actions: -200.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Episode reward after taking random actions: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your goal in the project will be to code an agent that can beat that ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Dyna\n",
    "\n",
    "4.1 State discretization\n",
    "In the mountain car environment, the state ranges from [âˆ’1.2, âˆ’0.07] to [0.6, 0.07]. To create a discrete state space, we divide the state space into bins. The bin sizes should differ for the two state dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 9]\n"
     ]
    }
   ],
   "source": [
    "num_bins = np.array([18,14]) # position and velocity\n",
    "\n",
    "def discretize_state(state, state_space, num_bins):\n",
    "    bin_sizes = (state_space.high - state_space.low) / num_bins\n",
    "    return ((state - state_space.low) / bin_sizes).astype(int), bin_sizes\n",
    "\n",
    "state = np.array([-0.5, 0.03])\n",
    "discrete_state,__ = discretize_state(state, env.observation_space, num_bins)\n",
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Model building**\n",
    "During training, the agent builds a model of the environment consisting of two key components: an estimation of transition probabilities PË†s,a(sâ€²) and an estimation of the reward for each state-action pair RË†(s, a). PË†s,a(sâ€²) represents the expected probability of transitioning to state sâ€² after taking action a in state s, while RË†(s, a) is the expected reward when taking action a in state s. These estimates are updated with each new observation.\n",
    "\n",
    "**4.3 Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make steps smaller (+ - didn't work)\n",
    "- visualise the car \n",
    "- check the Q-values\n",
    "- check the policy: doesn't always take the same action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "\n",
    "class DynaAgent:\n",
    "    def __init__(self, state_space, action_space, discr_step=np.array([0.025, 0.005]), gamma=0.99, epsilon=0.9, k=10):\n",
    "        self.discr_step = discr_step\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon \n",
    "        self.k = k\n",
    "        self.nstates = ((state_space.high - state_space.low) / self.discr_step).astype(int) + 1 # 1 is added to count upper bound as a separate state\n",
    "        self.nactions = action_space.n\n",
    "        self.P = np.ones((self.nstates[0], self.nstates[1], self.nactions, self.nstates[0], self.nstates[1])) / np.prod(self.nstates)\n",
    "        self.R = np.zeros((self.nstates[0], self.nstates[1], self.nactions))\n",
    "        self.Q = np.zeros((self.nstates[0], self.nstates[1], self.nactions))\n",
    "        self.visits = np.zeros((self.nstates[0], self.nstates[1], self.nactions), dtype=int)\n",
    "        self.learning_rate = 0.1\n",
    "        self.position_list = []\n",
    "        self.delta_t = 0.1\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        return np.floor((state - state_space.low) / self.discr_step).astype(int)\n",
    "\n",
    "    def update_model(self, s, a, r, s_prime):\n",
    "        #self.position_list.append(s_prime[0])\n",
    "        s_discrete = s #self.discretize_state(s)\n",
    "        s_prime_discrete = s_prime #self.discretize_state(s_prime)\n",
    "        self.visits[s_discrete[0], s_discrete[1], a] += 1\n",
    "        self.learning_rate = 1 / self.visits[s_discrete[0], s_discrete[1], a]\n",
    "        self.R[s_discrete[0], s_discrete[1], a] += self.learning_rate * (r - self.R[s_discrete[0], s_discrete[1], a])\n",
    "        self.P[s_discrete[0], s_discrete[1], a, s_prime_discrete[0], s_prime_discrete[1]] += self.learning_rate # controls how much the Q-values are updated at each step while moving towards the minimum of a loss function\n",
    "\n",
    "    def update_Q_value(self, s, a):\n",
    "        s_discrete =  s #self.discretize_state(s)\n",
    "        future_rewards = self.P[s_discrete[0], s_discrete[1], a] * np.max(self.Q, axis=2)\n",
    "        self.Q[s_discrete[0], s_discrete[1], a] = self.R[s_discrete[0], s_discrete[1], a] + self.gamma * np.sum(future_rewards)\n",
    "\n",
    "    def simulate(self):\n",
    "        s_prime = np.zeros(2, dtype=int)\n",
    "        for _ in range(self.k):\n",
    "            # Randomly select a state-action pair from those that have been visited\n",
    "            visited_states = np.where(self.visits > 0)\n",
    "            idx = np.random.choice(len(visited_states[0]))\n",
    "            s = (visited_states[0][idx], visited_states[1][idx])\n",
    "            a = visited_states[2][idx]\n",
    "\n",
    "            # Retrieve the model for this state-action pair\n",
    "            transition_probs= self.P[s[0], s[1], a]/ np.sum(self.P[s[0], s[1], a])\n",
    "            # Pick a new state randomly based on the transition probabilities\n",
    "            mat = np.repeat(np.arange(self.nstates[0])[:, np.newaxis], self.nstates[1], axis=1)\n",
    "            chosen_index = np.random.choice(mat.flatten(), p=transition_probs.flatten())\n",
    "            s_prime = np.unravel_index(chosen_index, shape=self.nstates)\n",
    "            # Retrieve the reward for this state-action pair\n",
    "            reward = self.R[s[0], s[1], a]\n",
    "            #if s_prime[1] > s[1]:\n",
    "            #    reward += 1\n",
    "\n",
    "            # Find the maximum Q-value for the next state across all possible actions\n",
    "            max_q_next = np.max(self.Q[s_prime[0], s_prime[1]])\n",
    "\n",
    "            # Update Q-value using the learning rate, reward, and discount factor\n",
    "            self.Q[s[0], s[1], a] += self.learning_rate * (reward + self.gamma * max_q_next - self.Q[s[0], s[1], a])\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.nactions)\n",
    "        else:\n",
    "            s_discrete = state #self.discretize_state(state)\n",
    "            return np.argmax(self.Q[s_discrete[0], s_discrete[1]])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= 0.99  # Exponential decay\n",
    "        self.epsilon = max(self.epsilon, 0.05)  # Ensuring epsilon does not go below 0.05\n",
    "\n",
    "    def render(self, file_path='./mountain_car.mp4', mode='mp4'):\n",
    "        \"\"\" When the method is called it saves an animation\n",
    "        of what happened until that point in the episode.\n",
    "        Ideally it should be called at the end of the episode,\n",
    "        and every k episodes.\n",
    "        \n",
    "        ATTENTION: It requires avconv and/or imagemagick installed.\n",
    "        @param file_path: the name and path of the video file\n",
    "        @param mode: the file can be saved as 'gif' or 'mp4'\n",
    "        \"\"\"\n",
    "        # Plot init\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, autoscale_on=False, xlim=(-1.2, 0.5), ylim=(-1.1, 1.1))\n",
    "        ax.grid(False)  # disable the grid\n",
    "        x_sin = np.linspace(start=-1.2, stop=0.5, num=100)\n",
    "        y_sin = np.sin(3 * x_sin)\n",
    "        # plt.plot(x, y)\n",
    "        ax.plot(x_sin, y_sin)  # plot the sine wave\n",
    "        # line, _ = ax.plot(x, y, 'o-', lw=2)\n",
    "        dot, = ax.plot([], [], 'ro')\n",
    "        time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "        _position_list = self.position_list\n",
    "        _delta_t = self.delta_t\n",
    "\n",
    "        def _init():\n",
    "            dot.set_data([], [])\n",
    "            time_text.set_text('')\n",
    "            return dot, time_text\n",
    "\n",
    "        def _animate(i):\n",
    "            x = _position_list[i]\n",
    "            y = np.sin(3 * x)\n",
    "            dot.set_data(x, y)\n",
    "            time_text.set_text(\"Time: \" + str(np.round(i*_delta_t, 1)) + \"s\" + '\\n' + \"Frame: \" + str(i))\n",
    "            return dot, time_text\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, _animate, np.arange(1, len(self.position_list)),\n",
    "                                    blit=True, init_func=_init, repeat=False)\n",
    "        if mode == 'gif':\n",
    "            ani.save(file_path, writer='imagemagick', fps=int(1/self.delta_t))\n",
    "        elif mode == 'mp4':\n",
    "            ani.save(file_path, fps=int(1/self.delta_t), writer='ffmpeg', codec='libx264')\n",
    "        # Clear the figure\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "    \n",
    "# Usage\n",
    "state_space = env.observation_space\n",
    "action_space = env.action_space  # Assuming 3 possible actions\n",
    "agent = DynaAgent(state_space, action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "total_reward = 0\n",
    "steps_per_episode = 200\n",
    "\n",
    "agent = DynaAgent(env.observation_space, env.action_space)\n",
    "\n",
    "while not done:\n",
    "    state, _ = env.reset()\n",
    "    agent.position_list = [state[0]]\n",
    "    state = agent.discretize_state(state)\n",
    "    total_reward = 0\n",
    "    for t in range(steps_per_episode):\n",
    "        # Direct learning from environment interactions\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "        agent.position_list.append(next_state[0])\n",
    "        # Incentivizing model to visit new states\n",
    "        if next_state[1] > state[0]:\n",
    "            reward += 1\n",
    "        \n",
    "        next_state = agent.discretize_state(next_state)\n",
    "        agent.update_Q_value(state, action) # ask why this is done before updating the model or try different configurations\n",
    "        agent.update_model(state, action, reward, next_state)\n",
    "\n",
    "        # Planning\n",
    "        agent.simulate()\n",
    "\n",
    "        total_reward += reward\n",
    "        #print(next_state)\n",
    "        state = next_state\n",
    "        #print(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/44g5ch495hdc8x4_5mq3dsdm0000gn/T/ipykernel_54123/557000056.py:159: MatplotlibDeprecationWarning: Setting data with a non sequence type is deprecated since 3.7 and will be remove two minor releases later\n",
      "  dot.set_data(x, y)\n"
     ]
    }
   ],
   "source": [
    "agent.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Though it looks simple, Mountaincar is actually pretty challenging because it's requires directed exploration. You exactly highlighted the difficulty: if it never sees the reward, it can't learn a good policy. Random exploration isn't enough, since the car won't gather the momentum needed.\n",
    "\n",
    "To have it see the reward, you need to somehow incentivize it to visit new parts of the environment. For example, you could bucket the state space, and add something to the reward if you get to a state you haven't seen many times before. This is called \"bonus-based exploration,\" and is an powerful general technique for learning in sparse-reward domains like this.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"That makes total sense, thank you.\n",
    "\n",
    "I did end up adding a conditional to the main loop to check if the current state had a higher acceleration compared to the previous states seen and then if it did I added a small amount to the reward before updating the value function. I don't know if this specific method makes sense but before I was thinking that this kind of manipulation outside of the agents main logic would be kind of \"cheating\" since it used code beyond the expected sarsa implementation.\"\n",
    "\n",
    "https://youtu.be/Lphlyuvocz0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    \n",
    "    def render(self, file_path='./mountain_car.mp4', mode='mp4'):\n",
    "        \"\"\"Saves an animation of the car's movement along the track.\n",
    "        \n",
    "        This method visualizes the positions recorded in `self.position_list`, showing a car moving along\n",
    "        a sinusoidal path. It should be called at the end of an episode or periodically.\n",
    "        \n",
    "        Requires `ffmpeg` for MP4. Ensure it is installed and correctly configured.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path and name of the video file.\n",
    "            mode (str): Format to save the video as 'gif' or 'mp4'.\n",
    "        \"\"\"\n",
    "        # Set up the plot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(-1.2, 0.5)\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Generate a sinusoidal track\n",
    "        x_sin = np.linspace(-1.2, 0.5, 100)\n",
    "        y_sin = np.sin(3 * x_sin)\n",
    "        ax.plot(x_sin, y_sin, label='Track')  # Plot the track\n",
    "\n",
    "        # Initialize the car as a red dot\n",
    "        car, = ax.plot([], [], 'ro', label='Car')\n",
    "        time_text = ax.text(0.05, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "        def init():\n",
    "            \"\"\"Initialize animation.\"\"\"\n",
    "            car.set_data([], [])\n",
    "            time_text.set_text('')\n",
    "            return car, time_text\n",
    "\n",
    "        def animate(i):\n",
    "            \"\"\"Update the animation by one frame.\"\"\"\n",
    "            x = self.position_list[i]\n",
    "            y = np.sin(3 * x)\n",
    "            car.set_data(x, y)\n",
    "            time_text.set_text(f\"Time: {np.round(i * self.delta_t, 1)}s\\nFrame: {i}\")\n",
    "            return car, time_text\n",
    "\n",
    "        # Create the animation\n",
    "        ani = animation.FuncAnimation(fig, animate, frames=len(self.position_list), init_func=init,\n",
    "                                      blit=True, repeat=False)\n",
    "\n",
    "        # Save the animation\n",
    "        if mode == 'gif':\n",
    "            ani.save(file_path, writer='imagemagick', fps=int(1 / self.delta_t))\n",
    "        elif mode == 'mp4':\n",
    "            ani.save(file_path, writer='ffmpeg', codec='libx264', fps=int(1 / self.delta_t))\n",
    "\n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
